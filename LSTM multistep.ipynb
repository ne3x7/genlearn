{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nickstulov/.virtualenvs/venv3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from livelossplot import PlotLosses\n",
    "from keras import backend as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display as dp\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.despine()\n",
    "\n",
    "dp.set_matplotlib_formats('retina')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stateful LSTM, reads `horizon` time steps, predicts next one for each, outputs all answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    \"\"\"\n",
    "    Batches generator.\n",
    "    \n",
    "    Takes a directory `path`, sequentially reads `batch_size` files, then\n",
    "        sequentially reads `horizon` timestamps from each file.\n",
    "        \n",
    "    Args:\n",
    "        path (str): Path to collection.\n",
    "        batch_size (int): Number of documents per batch.\n",
    "        horizon (int): Number of timestamps per batch.\n",
    "        dimension (int): Series vector length.\n",
    "    \"\"\"\n",
    "    def __init__(self, path, batch_size, horizon, dimension):\n",
    "        self._batch_size = batch_size\n",
    "        self._horizon = horizon\n",
    "        self._dimension = dimension\n",
    "        self._collection = [os.path.join(path, x) for x in os.listdir(path) if x != '.DS_Store']\n",
    "        self._collection_size = len(self._collection)\n",
    "        self._num_batches = self._collection_size // self._batch_size + 1\n",
    "        self._cursor_collection = 0\n",
    "        self._cursor_file = 0 jk\n",
    "        with open('sc.pickle', 'rb') as f:\n",
    "            self._sc = pickle.load(f)\n",
    "        self._read_next_files()\n",
    "        self._file_size = 125023\n",
    "        self._last_batch = self._read_from_files()\n",
    "        \n",
    "    def _read_next_files(self):\n",
    "        \"\"\"\n",
    "        Reads the collection by chunks to save memory.\n",
    "        \"\"\"\n",
    "        files_to_read = self._collection[self._cursor_collection : self._cursor_collection + self._batch_size]\n",
    "        self._cursor_collection += self._batch_size\n",
    "        if self._cursor_collection > self._collection_size:\n",
    "            diff = self._cursor_collection % self._collection_size\n",
    "            self._cursor_collection = diff\n",
    "            files_to_read.extend(self._collection[0 : diff])\n",
    "        self._files = [self._sc.transform(pd.read_csv(fpath).drop(['t'], axis=1).values) for fpath in files_to_read]\n",
    "        \n",
    "    def _read_from_files(self):\n",
    "        \"\"\"\n",
    "        Reads one timestamp for multiple series.\n",
    "        \n",
    "        Returns:\n",
    "            batch (numpy array): Array of shape (batch_size, 1, dimension).\n",
    "            reset (bool): Whether states should be reset.\n",
    "        \"\"\"\n",
    "        batch = np.zeros((self._batch_size, self._dimension))\n",
    "        for i, doc in enumerate(self._files):\n",
    "            batch[i] = doc[self._cursor_file]\n",
    "        self._cursor_file += 1\n",
    "        if self._cursor_file == self._file_size:\n",
    "            self._cursor_file = self._cursor_file % self._file_size\n",
    "            self._read_next_files()\n",
    "            return batch, True\n",
    "        else:\n",
    "            return batch, False\n",
    "        \n",
    "    def next(self):\n",
    "        \"\"\"\n",
    "        Reads multiple timestamps for multiple series, i.e. complete batch.\n",
    "        \"\"\"\n",
    "        batch = np.zeros((self._batch_size, self._horizon + 1, self._dimension))\n",
    "        if self._last_batch[1]:\n",
    "             self._last_batch = self._read_from_files()\n",
    "        batch[:, 0, :] = self._last_batch[0]\n",
    "        for i in range(1, self._horizon + 1):\n",
    "            b, reset = self._read_from_files()\n",
    "            batch[:, i, :] = b\n",
    "            if reset:\n",
    "                self._last_batch = self._last_batch[0], True\n",
    "                return batch, True\n",
    "        self._last_batch = batch[:, -1, :], False\n",
    "        return batch, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "batch_size = 100\n",
    "num_hidden = 100\n",
    "num_vars = 4\n",
    "horizon = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(batch_shape=(batch_size, horizon, num_vars))\n",
    "logits = LSTM(num_hidden, stateful=True, return_sequences=True, kernel_regularizer='l2',\n",
    "              recurrent_regularizer='l2', bias_regularizer='l2', activity_regularizer='l2',\n",
    "              recurrent_dropout=0.1)(inputs)\n",
    "outputs = TimeDistributed(Dense(num_vars))(logits)\n",
    "model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader('data', batch_size, horizon, num_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "liveplot = PlotLosses(figsize=(20,10))\n",
    "for epoch in range(int(1e4)):\n",
    "    batch, reset = dl.next()\n",
    "    hist = model.train_on_batch(batch[:, :-1, :], batch[:, 1:, :])\n",
    "    if reset:\n",
    "        model.reset_states()\n",
    "    liveplot.update({\n",
    "        'log_mse': np.log(hist)\n",
    "    })\n",
    "    liveplot.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveplot = PlotLosses(figsize=(20,10))\n",
    "for epoch in range(int(5e4)):\n",
    "    batch, reset = dl.next()\n",
    "    hist = model.train_on_batch(batch[:, :-1, :], batch[:, 1:, :])\n",
    "    if reset:\n",
    "        model.layers[1].reset_states()\n",
    "    liveplot.update({\n",
    "        'mse': hist\n",
    "    })\n",
    "    liveplot.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = pd.read_csv('data_val/data_80.csv').drop(['t'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sc.pickle', 'rb') as f:\n",
    "    sc = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = sc.transform(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdiffs = np.array([vals[i-1, 0] - vals[i, 0] for i in range(1, len(vals))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks = np.where(np.abs(vdiffs) > np.mean(np.abs(vdiffs)) + 2 * np.std(np.abs(vdiffs)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks_on_off = [(ticks[i], ticks[i+1]) for i in range(len(ticks)) if i % 2 == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticks_on, ticks_off = tuple(np.array(ticks_on_off).T.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tnrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подать 60, первые 30 это ничего, дальше по одному шажку получить state для сдвига на один\n",
    "# цель: иметь state для k * horizon + i шага \\forall i \\in [0, horizon)\n",
    "# для этого нужно иметь предысторию размером horizon и сохраненный state в позиции (k-1) * horizon + i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_states()\n",
    "reset_states = model.layers[1].states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = [*vals[:horizon]]\n",
    "states = [reset_states for _ in range(horizon)]\n",
    "disturbance = False\n",
    "for i in tnrange(horizon, len(vals) // horizon * horizon, horizon):\n",
    "    for j in range(horizon):\n",
    "        inputs = np.zeros((batch_size, horizon, num_vars))\n",
    "        state = states[i-horizon+j]\n",
    "        model.layers[1].states[0].assign(state[0].value())\n",
    "        model.layers[1].states[1].assign(state[1].value())\n",
    "        if i+j in ticks_on:\n",
    "            disturbance = True\n",
    "        elif i+j in ticks_off:\n",
    "            disturbance = False\n",
    "        if disturbance:\n",
    "            inputs[0, :-1] = predicted[i-horizon+j:i+j-1]\n",
    "            inputs[0, -1] = vals[i+j]\n",
    "        else:\n",
    "            inputs[0] = predicted[i-horizon+j:i+j]\n",
    "        pred = model.predict_on_batch(inputs)[0][-1]\n",
    "        predicted.append(pred)\n",
    "        states.append(model.layers[1].states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = np.array(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_pred = np.array(sc.inverse_transform(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_true = sc.inverse_transform(vals)[:150030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(concat_pred.shape, concat_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = ['$V$', '$\\\\varphi$', '$P$', '$Q$']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(20,20))\n",
    "fig.tight_layout(h_pad=4.0)\n",
    "for i, (ax, name) in enumerate(zip(axes.flatten(), var_names)):\n",
    "    ax.plot(concat_pred[:, i], c='r', label='pred')\n",
    "    ax.plot(concat_true[:, i], c='b', label='true')\n",
    "    ax.set_title(name, fontsize='xx-large')\n",
    "    ax.legend(loc='best', fontsize='xx-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(20,20))\n",
    "fig.tight_layout(h_pad=4.0)\n",
    "for i, (ax, name) in enumerate(zip(axes.flatten(), var_names)):\n",
    "    ax.plot(concat_pred_new[:, i], c='r', label='pred')\n",
    "    ax.plot(concat_true[:, i], c='b', label='true')\n",
    "    ax.set_title(name, fontsize='xx-large')\n",
    "    ax.legend(loc='best', fontsize='xx-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(20,20))\n",
    "fig.tight_layout(h_pad=4.0)\n",
    "for i, (ax, name) in enumerate(zip(axes.flatten(), var_names)):\n",
    "    ax.plot(((np.repeat(concat_true[0, i], ???) - concat_true[:, i]) / concat_true[:, i]) ** 2, 'g', label='pers')\n",
    "    ax.plot(((concat_pred[:, i] - concat_true[:, i]) / concat_true[:, i]) ** 2, 'r', label='pred')\n",
    "    ax.set_title(name, fontsize='xx-large')\n",
    "    ax.legend(loc='best', fontsize='xx-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(np.mean(((concat_true[:-1] - concat_true[1:]) / concat_true[1:]) ** 2, axis=1), 'g', label='pers')\n",
    "plt.plot(np.mean(((concat_pred - concat_true) / concat_true) ** 2, axis=1), 'r', label='pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('lstm_scaled_50k.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nrmse(y_true, y_pred):\n",
    "    return K.mean(K.sqrt(K.mean(K.sum((y_true - y_pred) ** 2, axis=2), axis=1)) \\\n",
    "                  / K.sqrt(K.mean(K.sum(y_true ** 2, axis=2), axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = load_model('lstm_supervised_scaled2_50k.h5', custom_objects={'nrmse': nrmse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(m, show_layer_names=False, rankdir='LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
